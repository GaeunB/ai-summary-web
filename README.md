# ğŸ‘‹ì•ˆë…•í•˜ì„¸ìš”, 'ìì†Œì„œ ìš”ì•½ë‹¨'ì…ë‹ˆë‹¤.
**íŒ€ì›**: ë³€ê°€ì€, ê¹€ì§€í™˜, ë‚¨ê²½í˜„, ì´ëŒ€í—Œ, ì´ì„±ë¯¼, ìµœë¯¼ì¬<br />
**íŒ€ë…¸ì…˜í˜ì´ì§€**: [Notion](https://jungle-crane-580.notion.site/27be448bebcb4ca39ac9182033d7a293?pvs=4)
<br />
<img src="https://github.com/GaeunB/read_/assets/115343409/8b8a19aa-bae9-473b-9875-37f65656e2c4" height="1200"/>


## ğŸ” Table of Contents
+ About
+ Main Fuctions
+ Architecture
+ Setting
+ Engine
  + General_Summarization
  + Keysentence_Extraction
  + Question_Answering
+ Code_Architecture

## ğŸ“ About
ğŸ’¡**ì£¼ì œ**: ì¸ê³µì§€ëŠ¥ í™œìš© ìê¸°ì†Œê°œì„œ ìš”ì•½ í˜ì´ì§€ <br />
`#NLP` `#Text_Summarization` `#Question_Answering` `#Flask` <br />
<aside>

- **ê°œìš”**  <br />
	<img src="https://github.com/GaeunB/read_/assets/115343409/a0306374-660e-4f03-bb69-e2daa384bb56">

    - ë§¤ ì±„ìš©ì‹œ ì§€ì›ìëŠ” ë™ì¼í•œ ì§ˆë¬¸ì„ ì œí•œëœ ê¸€ì ìˆ˜ ë‚´ì—ì„œ ë‹µë³€ì„ í•˜ê³  ìˆë‹¤.
    - ì‹ ì…ì‚¬ì› ì±„ìš©ì€ ì´ë ¥ì„œë³´ë‹¤ ìê¸°ì†Œê°œì„œë¥¼ ì¤‘ìš”í•˜ê²Œ í‰ê°€
        - ê²½ë ¥ì´ ì—†ëŠ” ì‹ ì…ì‚¬ì›ì˜ ì—…ë¬´ ìˆ˜í–‰ëŠ¥ë ¥ì€ ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì— ì§€ì›ìì˜ ì ì¬ë ¥ì„ íŒŒì•…í•˜ê¸° ìœ„í•œ ë„êµ¬ë¡œ ìê¸°ì†Œê°œì„œê°€ í° ë¹„ì¤‘ì„ ì°¨ì§€í•œë‹¤.

	 â‡’â€˜**ìì†Œì„œ ìš”ì•½ë‹¨**â€™ì€ ê¸°ì—… ì„ì›ì§„ì„ ëŒ€ìƒìœ¼ë¡œ ìê¸°ì†Œê°œì„œì˜ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•˜ê³ , ìƒí˜¸ì‘ìš©í•  ìˆ˜ ìˆëŠ” ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ì˜ ìê¸°ì†Œê°œ ìš”ì•½ í˜ì´ì§€ë¥¼ ì œê³µí•œë‹¤.

</aside>
<br />

- **ëœë”© í˜ì´ì§€**

![ëœë”©í˜ì´ì§€@3x height='200'](https://github.com/GaeunB/read_/assets/115343409/b701310c-9fda-4647-8f42-b9bd59d7c0c5)

- **ì‹œì—° ì˜ìƒ**
<br />
<br />

## ğŸš¦ Main Fuctions
- **General Summarization**: ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì„ í™œìš©í•œ ìê¸°ì†Œê°œì„œ ìš”ì•½
- **Keysentence Extraction**: í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ë¬¸ì¥ ì¶”ì¶œ
- **Question Answering**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì œê³µ 

## ğŸ”§ Architecture
+ **Frontend**: HTML, CSS, JS <br />
+ **Engine** : Pytorch <br /> 
	+ Kobart, Textrankr, Bert 
+ **Backend**: Flask <br />

<br />ì „ì²´ êµ¬ì¡° ë° ì‘ë™ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 
<br /> (ë‹¤ì´ì–´ê·¸ë¨)
## ğŸ’¾ Setting
- **Install modules**
```bash
pip install -r requirements.txt 
``` 
- **Execute**
```bash
flask run
```

## âš¡ Engine
#### âœ… General Summarization_kobart
https://github.com/SKT-AI/KoBART <br /> 
- BART(Bidirectional and Auto-Regressive Transformers)ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ ì¼ë¶€ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì´ë¥¼ ë‹¤ì‹œ ì›ë¬¸ìœ¼ë¡œ ë³µêµ¬í•˜ëŠ” autoencoderì˜ í˜•íƒœë¡œ í•™ìŠµì´ ë©ë‹ˆë‹¤.<br /> 
![image width=650 height=550](https://github.com/Oneourbefore/ai-summary-web/assets/102707496/68021f7e-9981-4a97-9ae3-b3e6ff830132)
1. BartëŠ” Transformerì˜ ê¸°ë³¸ ì•„í‚¤í…ì²˜ì¸ Encoder-Decoderêµ¬ì¡°ë¥¼ ê°–ê³  ìˆë‹¤.
2. ë”°ë¼ì„œ ì½”ë“œë„ Encoderì™€ Decoderë¥¼ ì°¨ë¡€ë¡œ í†µê³¼í•œë‹¤.
3. Input dataë„ Encoder_inputê³¼ Decoder_inputì„ ë”°ë¡œ ì¤€ë¹„í•´ì•¼í•œë‹¤.
4. ì–´ë–»ê²Œ inputì„ ë„£ì–´ì£¼ëƒì— ë”°ë¼ Taskë§ˆë‹¤ í•™ìŠµ/ì¶”ë¡  ë°©ë²•ì´ ê°ˆë¦°ë‹¤.

- í•œêµ­ì–´ BARTëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ Text Infilling ë…¸ì´ì¦ˆ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 40GB ì´ìƒì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œ í•™ìŠµí•œ í•œêµ­ì–´ encoder-decoder ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. <br /> 
ì´ë¥¼ í†µí•´ ë„ì¶œëœ KoBART-baseë¥¼ ë°°í¬í•©ë‹ˆë‹¤. 
í•œêµ­ì–´ ìœ„í‚¤ ë°±ê³¼ ì´ì™¸, ë‰´ìŠ¤, ì±…, ëª¨ë‘ì˜ ë§ë­‰ì¹˜ v1.0(ëŒ€í™”, ë‰´ìŠ¤, ...), ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë“±ì˜ ë‹¤ì–‘í•œ ë°ì´í„°ê°€ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

- KoBARTë€ í˜ì´ìŠ¤ë¶ì—ì„œ ê³µê°œí•œ BARTëª¨ë¸ì„ SKTì—ì„œ 40GBì´ìƒì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¡œ ì‚¬ì „ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì´ë‹¤.<br /> 
BARTëŠ” seq2seq ëª¨ë¸ì„ ì‚¬ì „í•™ìŠµí•˜ê¸° ìœ„í•œ denoising autoencoder(DAE, ì¡ìŒì œê±° ì˜¤í†  ì¸ì½”ë”)ë¡œ, ì„ì˜ì˜ noising functionìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì†ìƒì‹œí‚¨ í›„ ëª¨ë¸ì´ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì¬êµ¬ì¶•í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ëœë‹¤.<br /> 
BARTëŠ” ê¸°ì¡´ BERTëª¨ë¸ê³¼ GPTë¥¼ í•©ì¹œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆëŠ”ë°, ì´ë¡œ ì¸í•´ BERTì˜ Bidirectional íŠ¹ì§•ê³¼ GPTì˜ Auto-Regressiveí•œ íŠ¹ì§•ì„ ëª¨ë‘ ê°€ì§„ë‹¤. ë•ë¶„ì— BARTëŠ” ê¸°ì¡´ MLMëª¨ë¸ë“¤ì— ë¹„í•´ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë†’ì€ í™œìš©ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤. 
![image width=650 height=550](https://github.com/GaeunB/ai-summary-web/assets/118701576/f2e5c8e6-cedc-4ccf-b9f2-90f6ccc25501 ) <br /> 
**Fig.1 Bartêµ¬ì¡°**<br /> 
- BARTëŠ” ì†ìƒëœ Textë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ Bidirectional ëª¨ë¸ë¡œ encodingí•˜ê³  ì •ë‹µ Textì— ëŒ€í•œ likelihoodë¥¼ autoregressive ëª¨ë¸ë¡œ decodingí•˜ì—¬ ê³„ì‚°í•œë‹¤. 
BARTì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ 5ê°€ì§€ noising ê¸°ë²•ì´ ì¡´ì¬í•œë©°, ì´ë¥¼ í†µí•´ ì†ìƒëœ Textë¥¼ ì–»ëŠ”ë‹¤.
![image width=650 height=550](https://github.com/GaeunB/ai-summary-web/assets/118701576/d4db2cc4-2686-40e3-b4dd-38312d0c3ff1) <br /> 
**Fig.2 Noisingê¸°ë²•**<br /> 
- BARTëŠ” ìê¸°íšŒê·€ ë””ì½”ë”ë¥¼ ê°–ê¸° ë•Œë¬¸ì—, abstractive QAì™€ summarizationê³¼ ê°™ì€ ì‹œí€€ìŠ¤ ì¼ë°˜í™”(Sequence Generation) íƒœìŠ¤í¬ì— ì§ì ‘ì ìœ¼ë¡œ íŒŒì¸íŠœë‹ ë  ìˆ˜ ìˆë‹¤. ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì´ë ¥ì„œ ìš”ì•½ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ KoBARTëª¨ë¸ì— ì±„ìš©ë©´ì ‘ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ì˜€ë‹¤.(ë°ì´í„°ì…‹: https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=71592) 
<br />

<details><summary>ì°¸ê³ ë¬¸í—Œ</summary>
[1] Mike Lewisì™¸(2019), "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", ACL <br /> 
[2] ìˆ˜ë‹¤ë¥´ì‚° ë¼ë¹„ì°¬ë””ë€(2021), "êµ¬ê¸€ BERTì˜ ì •ì„", í•œë¹›ë¯¸ë””ì–´
</details>


#### âœ… Keysentence Extraction_Textrankr
- TextRank ì•Œê³ ë¦¬ì¦˜ì€ 2004ë…„ êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ PageRank ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤[1].<br />
PageRank ì•Œê³ ë¦¬ì¦˜ì€ ìˆ˜ì§‘ëœ ì¸í„°ë„· ë¬¸ì„œ ê°ê°ì„ ê·¸ë˜í”„ì˜ ë…¸ë“œ, ë¬¸ì„œ ë‚´ë¶€ì˜ ë§í¬ ì •ë³´ë¥¼ ê°„ì„ ìœ¼ë¡œ ê°€ì •í•˜ì—¬ ë°©í–¥ì„±ì´ ìˆëŠ” ê·¸ë˜í”„ë¥¼ ë§Œë“¤ì–´ ë¬¸ì„œì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•œë‹¤[2]. ì¡°ê¸ˆ ë” ì‰½ê²Œ ë§í•˜ìë©´ PageRankëŠ” ê° ì›¹í˜ì´ì§€ë§ˆë‹¤ í•˜ì´í¼ë§í¬ê°€ ìˆì„ ë•Œ ì–¼ë§ˆë‚˜ ë§í¬ë¥¼ ë°›ëŠëƒì— ë”°ë¼ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ë§í•œë‹¤. ì¦‰, í•´ë‹¹ ë§í¬ë¥¼ í´ë¦­í•  í™•ë¥ ë¡œ ê·¸ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ê²ƒì´ë‹¤.<br />
- TextRank ì•Œê³ ë¦¬ì¦˜ì€ PageRankì˜ ê°œë…ì„ ìì—°ì–´ ì²˜ë¦¬ì— ì‘ìš©í•œ ê²ƒìœ¼ë¡œ ë¬¸ì¥, ë‹¨ì–´ì™€ ê°™ì€ íŠ¹ì • ë‹¨ìœ„ë“¤ ê°„ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ë¬¸ì„œ ë‚´ì˜ ê° ë¬¸ì¥ì„ ê·¸ë˜í”„ì˜ ì •ì (vertex)ìœ¼ë¡œ ê°€ì •í•˜ëŠ” ê²½ìš° ì¤‘ìš”í•œ ë¬¸ì¥ë“¤ì„ ì„ ë³„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ë¬¸ì„œ ìš”ì•½ì´ ê°€ëŠ¥í•˜ë‹¤. ê²°êµ­, TextRankëŠ” ì•ì„œ PageRankì—ì„œì˜ í˜ì´ì§€ ê°œë…ì„ ë‹¨ì–´ì˜ ê°œë…ìœ¼ë¡œ ë°”ê¾¼ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. í…ìŠ¤íŠ¸ë¡œ ì´ë£¨ì–´ì§„ ê¸€ì—ì„œ íŠ¹ì • ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë¬¸ì¥ê³¼ ì–¼ë§ˆë§Œí¼ì˜ ê´€ê³„ë¥¼ ë§ºê³  ìˆëŠ”ì§€ë¥¼ ê³„ì‚°í•œë‹¤.<br />
<img width="450" alt="ìŠ¤í¬ë¦°ìƒ· 2024-02-20 ì˜¤ì „ 11 22 32" src="https://github.com/28sungmin/study/assets/115343559/e807a8ef-b229-404d-8be0-3059ce4304cd">
<br />

- ìœ„ ì´ë¯¸ì§€ëŠ” ì£¼ì–´ì§„ ê¸€ì— ëŒ€í•´ í…ìŠ¤íŠ¸ ê°„ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ë¡œ ê·¸ë¦° ìƒ˜í”Œ ì´ë¯¸ì§€ì´ë‹¤. ê° ë¬¸ì¥ì—ì„œ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì„ ìœ¼ë¡œ ì—°ê²°í•œ ê²ƒì´ë‹¤.<br />
ê·¸ë ‡ë‹¤ë©´ í•µì‹¬ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ? ì•„ë˜ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë¬¸ì¥ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ê° ë¬¸ì¥ì´ ë§ˆë””ê°€ ë˜ëŠ” ê²ƒì´ë‹¤.
<img width="450" alt="ìŠ¤í¬ë¦°ìƒ· 2024-02-20 ì˜¤ì „ 11 28 52" src="https://github.com/28sungmin/study/assets/115343559/0e6bf9db-ab64-4ece-bac8-1b77cd2a277c">
<br />ì •ì„ì› ì™¸(2017) "TextRank ì•Œê³ ë¦¬ì¦˜ê³¼ ì£¼ì˜ ì§‘ì¤‘ ìˆœí™˜ ì‹ ê²½ë§ì„ ì´ìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¬¸ì„œ ìš”ì•½" ë…¼ë¬¸ì—ì„œëŠ” TextRank ì•Œê³ ë¦¬ì¦˜ì€ ê° ë¬¸ì¥ì˜ ì¤‘ìš”ë„ë¥¼ êµ¬í•  ë•Œ, ë¬¸ì¥ ê°„ ìƒê´€í–‰ë ¬ì„ ì´ìš©í•˜ì—¬ êµ¬í•˜ì˜€ë‹¤.
<img width="400" alt="textrank1" src="https://github.com/28sungmin/study/assets/115343559/b4f130bc-a2ac-40f8-bd2a-cf7a73ddd75c"><br />

- ì…ë ¥ ë¬¸ì„œì˜ ê° ë¬¸ì¥ë“¤ì— ëŒ€í•´ í˜•íƒœì†Œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , ì²´ì–¸ë¥˜ì™€ ìš©ì–¸ë¥˜ì˜ TF-IDFë¥¼ ê³„ì‚°í•˜ì—¬ ë¬¸ì¥-ë‹¨ì–´ í–‰ë ¬ì„ ìƒì„±í•œë‹¤. ê·¸ ë’¤ ìƒì„±ëœ ë¬¸ì¥-ë‹¨ì–´ í–‰ë ¬ì˜ ì „ì¹˜ í–‰ë ¬ì„ êµ¬í•˜ì—¬ ì„œë¡œ ê³±í•´ì£¼ë©´ ë¬¸ì¥ ê°„ì˜ ìƒê´€ê´€ê³„(correlation)ì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ êµ¬í•œ ë¬¸ì¥ ê°„ ìƒê´€í–‰ë ¬ì€ ë¬¸ì¥ ê°„ì˜ ê°€ì¤‘ì¹˜ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê³ , TextRank ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ê° ë¬¸ì¥ì˜ ì¤‘ìš”ë„ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ êµ¬í•œ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ë¬¸ì¥ë“¤ì„ ì •ë ¬í•œ ë’¤ ìƒìœ„ nê°œì˜ ë¬¸ì¥ë“¤ì„ ì¬ë°°ì¹˜í•˜ë©´ ìš”ì•½ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤[3].<br />

<br />
<details><summary>ì°¸ê³ ë¬¸í—Œ</summary>
[1] ì´ìƒì˜ ì™¸(2023), "TextRank ì•Œê³ ë¦¬ì¦˜ ë° ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•œ ë¸Œë ˆì¸ìŠ¤í† ë°", JPEE : Journal of practical engineering education = ì‹¤ì²œê³µí•™êµìœ¡ë…¼ë¬¸ì§€, v.15 no.2, pp.509 - 517 <br />
[2] ë°°ì›ì‹ê³¼ ì°¨ì •ì›(2010), "TextRank ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ë¬¸ì„œ ë²”ì£¼í™”", ì •ë³´ê³¼í•™íšŒë…¼ë¬¸ì§€. Journal of KIISE. ì»´í“¨íŒ…ì˜ ì‹¤ì œ ë° ë ˆí„°, v.16 no.1, pp.110-114 <br />
[3] ì •ì„ì› ì™¸(2017), "TextRank ì•Œê³ ë¦¬ì¦˜ê³¼ ì£¼ì˜ì§‘ì¤‘ ìˆœí™˜ ì‹ ê²½ë§ì„ ì´ìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¬¸ì„œ ìš”ì•½", í•œêµ­ì–´ì •ë³´í•™íšŒ 2017ë…„ë„ ì œ29íšŒ í•œê¸€ë°í•œêµ­ì–´ì •ë³´ì²˜ë¦¬í•™ìˆ ëŒ€íšŒ, pp.47 - 50 <br />
</details>


#### âœ… Question Answering_Bert (ì›ë¦¬)
-Question Answering modelì€ ì‚¬ìš©ìë¡œë¶€í„° ë°›ì€ íŠ¹ì •í•œ ì§ˆì˜ì— ê´€ë ¨ëœ ì •ë‹µì„ ìì—°ì–´ë¡œ ìë™ìœ¼ë¡œ ì¶œë ¥í•˜ëŠ” ì‹œìŠ¤í…œì´ë‹¤. [1] 

QA ì‘ì—…ì€ ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì„ ì´í•´í•˜ì—¬ ë‹µë³€ì„ ì œì‹œí•˜ëŠ” ê²ƒì´ ëª©í‘œì¸ë°, ë‹µë³€ì„ ì°¾ëŠ” ë°©ì‹ì— ë”°ë¼ ì¶”ì¶œí˜•(extractive), ì¶”ìƒí˜•(abstractive) ìœ¼ë¡œ ë‚˜ë‰œë‹¤. ë¬¸ì¥ ë‚´ì—ì„œ ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ë‹µë³€ì„ ì°¾ì•„ë‚´ëŠ”ì§€ / ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì§ì ‘ ìƒì„±í•˜ëŠ” ë°©ì‹ì˜ ì°¨ì´ì´ë‹¤. QA ì‘ì—…ì„ ì§„í–‰í•˜ê¸° ìœ„í•´, êµ¬ê¸€ì—ì„œ ê°œë°œí•œ NLP ì²˜ë¦¬ ëª¨ë¸ì¸ BERT ëª¨ë¸ì„ í™œìš©í•˜ì˜€ë‹¤. 

BERT ëª¨ë¸ì€ ì‚¬ì „ í›ˆë ¨ ì–¸ì–´ ëª¨ë¸ë¡œì„œ, íŠ¹ì • ë¶„ì•¼ì— êµ­í•œëœ ê¸°ìˆ ì´ ì•„ë‹ˆë¼ ëª¨ë“  ìì—°ì–´ ì²˜ë¦¬ ë¶„ì•¼ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ë²”ìš© Language Modelì´ë‹¤. Transformer architertureì—ì„œ encoderë§Œ ì‚¬ìš©í•œë‹¤ëŠ” íŠ¹ì§•ì´ ìˆë‹¤.êµ¬ì¡°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

##### 1. Input 

![image](https://github.com/GaeunB/ai-summary-web/assets/145184645/fdf376d2-bda1-43b5-8341-f290b3a490c9){: width="100" height="100"}

Inputì€ Token Embedding + Segment Embedding + Position Embedding 3ê°€ì§€ ì„ë² ë”©ì„ ê²°í•©í•œ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•œë‹¤.
-Token Embedding : Word Piece ì„ë² ë”© ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. Char ë‹¨ìœ„ë¡œ ì„ë² ë”© í›„ ë“±ì¥ ë¹ˆë„ì— ë”°ë¼ sub-word ë¡œ êµ¬ë¶„í•œë‹¤.
-Segment Embedding : Sentence Embeddingìœ¼ë¡œ, í† í° ì‹œí‚¨ ë‹¨ì–´ë“¤ì„ ë‹¤ì‹œ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“œëŠ” ì‘ì—…ì´ë‹¤. êµ¬ë¶„ì [SEP] ë¥¼ í†µí•´ ë¬¸ì¥ì„ êµ¬ë¶„í•˜ê³  ë‘ ë¬¸ì¥ì„ í•˜ë‚˜ì˜ Segment ë¡œ ì§€ì •í•œë‹¤. 
-Position Embedding : ì…ë ¥ í† í°ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³ , í† í° ìˆœì„œëŒ€ë¡œ ì¸ì½”ë”©í•œë‹¤. 

##### 2. Pre-Training

!<img src="https://github.com/GaeunB/ai-summary-web/assets/145184645/56f14403-b877-4de7-b0a3-5278a01910b0" width="50%" height="50%">

-MLM(masked Language Model) : ì…ë ¥ ë¬¸ì¥ì—ì„œ ì„ì˜ë¡œ í† í°ì„ ë²„ë¦¬ê³ (Mask) ê·¸ í† í°ì„ ë§ì¶”ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤. 

-NSP(Next Sentence Prediction) : ë‘ ë¬¸ì¥ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ë‘ ë¬¸ì¥ì˜ ìˆœì„œë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì´ë‹¤. ë‘ ë¬¸ì¥ ê°„ ê´€ë ¨ì´ ê³ ë ¤ë˜ì•¼ í•˜ëŠ” NLIì™€ QAì˜ íŒŒì¸ íŠœë‹ì„ ìœ„í•´ ë‘ ë¬¸ì¥ì˜ ì—°ê´€ì„ ë§ì¶”ëŠ” í•™ìŠµì„ ì§„í–‰í•œë‹¤.

#### 3. Fine-tuning

![image](https://github.com/GaeunB/ai-summary-web/assets/145184645/100973f4-d8ea-4ac3-bc8e-b014a0a45e56){: width="100" height="100"}

BertëŠ” fine-tuning ë‹¨ê³„ì—ì„œ pre-training ê³¼ ê±°ì˜ ë™ì¼í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•œë‹¤. ê° NLP taskë§ˆë‹¤ fine-tuning í›„ Bert ëª¨ë¸ì„ transfer learningì‹œì¼œ ì„±ëŠ¥ì„ í™•ì¸í•œë‹¤.

BERT ê¸°ë°˜ QA ëª¨ë¸ì€ í¬ê²Œ ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤. 
1. ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì„ í•¨ê»˜ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹µë³€ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹
2. ì§ˆë¬¸ê³¼ ë¬¸ë§¥ì„ ê°ê° ë”°ë¡œ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê°ê°ì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•˜ê³ , ì´ë¥¼ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ë‹µë³€ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ 
ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” questionê³¼ referenceë¥¼ ë™ì‹œì— input ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ extractive question answering ëª¨ë¸ì„ êµ¬ì„±í•˜ì˜€ë‹¤.
![image](https://github.com/GaeunB/ai-summary-web/assets/145184645/4f0335a4-1310-46ac-a5be-55c3609d5b05)

<details><summary>ì°¸ê³ ë¬¸í—Œ</summary>
[1] ê¶Œì„¸ë¦°, et al. "ì˜ë£Œ ê´€ë ¨ ì§ˆì˜ì‘ë‹µì„ ìœ„í•œ BERT ê¸°ë°˜ í•œêµ­ì–´ QA ëª¨ë¸." Proceedings of KIIT Conference. 2022. <br />
[2] Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018). <br />
[3] ì†¡ë‹¤ì—°, ì¡°ìƒí˜„, and ê¶Œí˜ì² . "í•œêµ­ì–´ ë¬¸ë²• QA ì‹œìŠ¤í…œì„ ìœ„í•œ BERT ê¸°ë°˜ ì‹œí€€ìŠ¤ ë¶„ë¥˜ëª¨ë¸." í•œêµ­ì •ë³´ê³¼í•™íšŒ í•™ìˆ ë°œí‘œë…¼ë¬¸ì§‘ (2021): 754-756. <br />
</details>

## ğŸ“‘ Code Architecture
**âœ”ï¸ `app.py`** <br />
- `app.py` ì—ì„œëŠ” ì—”ì§„ì„ ì´ˆê¸° ì„¸íŒ…í•˜ê³ , flask ì„œë²„ ì„¤ì •ì„ í†µí•´ í”„ë¡ íŠ¸ì™€ í†µì‹ ì„ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ì—”ì§„ì—ì„œ ê°€ì¥ ì¤‘ì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ì½”ë“œ íŒŒì¼ì´ë©°, ë°±ì—”ë“œì—ì„œ ìš”ì²­ì´ ë“¤ì–´ì˜¬ ê²½ìš°, ê·¸ì— ë§ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•˜ì—¬ ì²˜ë¦¬í•œë‹¤.
- flask_corsë¥¼ í™œìš©í•˜ì—¬ CORS ì´ìŠˆë¥¼ í•´ê²°í•œë‹¤.<br />

```Python
from flask import Flask,render_template,request, jsonify
from flask_cors import CORS
import torch
from sum_model import summarize_model
from ext import textrank_summarize
from qa_model import get_qa_model
	
app = Flask(__name__)
cors = CORS(app)
```

- flask ì•±ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ê²½ë¡œì— ëŒ€í•œ ë¼ìš°íŒ…ì„ ì„¤ì •í•œë‹¤.
	- `'/'`: í™ˆ í˜ì´ì§€ë¥¼ ë Œë”ë§
	- `'/sum'`: ìš”ì•½ í˜ì´ì§€ë¥¼ ë Œë”ë§
	- `'/sum/gsummarize'`: ì¼ë°˜ì ì¸ ìš”ì•½ì„ ìˆ˜í–‰
	- `'/sum/key'`: í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ë¬¸ì¥ ì¶”ì¶œì„ ìˆ˜í–‰
	- `'/sum/qa'`: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µ

```Python
#home
@app.route('/')	
def home():
	return render_template('home.html')
	
#summary page
@app.route('/sum')	
def index():
	return render_template('index.html')
	
#general summarization
@app.route('/sum/gsummarize', methods=['POST'])
def gsummarize():
	try:
		data = request.get_json(force=True)
		context = data['context']
		gsum = summarize_model(context)
		response = jsonify({'gsum': gsum})
	
		except Exception as e:
			response = jsonify({'error': str(e)})	
		return response
	
	
# keysentence extraction
@app.route('/sum/key',methods=['POST'])
def key():
	try:
		data = request.get_json(force=True)
		context = data['context']
		keytext = textrank_summarize(context,1) #ë¬¸ì¥ ìˆ˜ ì¡°ì ˆ í•„ìš”
		response = jsonify({'keytext': keytext})
	
		except Exception as e:
			response = jsonify({'error': str(e)})	
			
		return response
		
#qa
@app.route('/sum/qa', methods=['POST'])
def qa_endpoint():
	try:
		data = request.get_json(force=True)
	
		context = data['context']
		question = data['question']
		if question == "":
			response = jsonify({'error': 'ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'})
			return response
	
			to_predict = [{"context": context, "qas": [{"question": question, "id": "0"}]}]
	
			result = qa_model.predict(to_predict)
	
			answer = result[0][0]['answer'][0]
			answer = "ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." if answer == '' else answer
	
			response = jsonify({'answer': answer})
	
		except Exception as e:
			response = jsonify({'error': str(e)})
	
		return response
```
- ì§€ì •ëœ ëª¨ë¸ì˜ ê²½ë¡œë¡œë¶€í„° CUDAë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ëª¨ë¸ì„ ë¡œë“œí•˜ë©°, í•´ë‹¹ ëª¨ë¸ì€ `'qa_model'` ë³€ìˆ˜ì— ì €ì¥ëœë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, í¬íŠ¸ '5000'ì—ì„œ flask ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹¤í–‰ì‹œí‚¨ë‹¤.

```Python
if __name__ == '__main__':
	model_path = 'model/checkpoint-1119-epoch-1'
	qa_model = get_qa_model(model_path, use_cuda=False)
	
	app.run(host='127.0.0.1',port=5000,debug=True)
	
```


**âœ”ï¸ `ext.py`**<br />
- í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì¤‘ìš”ë¬¸ì¥ì„ ì¶”ì¶œí•˜ëŠ”ëŠ” ëª¨ë“ˆì´ë‹¤.<br />
- ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬:<br />
`konlpy`: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ <br />
`textrankr`: TextRank ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•œ í…ìŠ¤íŠ¸ ìš”ì•½ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

```Python
from typing import List
from konlpy.tag import Okt
from textrankr import TextRank
	
class OktTokenizer:
okt: Okt = Okt()

def __call__(self, text: str) -> List[str]:
	tokens: List[str] = self.okt.phrases(text)
	return tokens
	
def textrank_summarize(text: str, num_sentences: int, verbose: bool = True) -> str:
	mytokenizer: OktTokenizer = OktTokenizer()
	textrank: TextRank = TextRank(mytokenizer)
	summarized: str = textrank.summarize(text, num_sentences, verbose)
	return summarized
```


**âœ”ï¸ `qa_model.py`**<br />
- ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ëª¨ë“ˆì´ë‹¤.<br />
- ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬:
	- `simpletransformers`: ê°„í¸í•œ ì‚¬ìš©ì„ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬<br />

```Python
import simpletransformers
	
from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs
	
def get_qa_model(model_path, use_cuda=False):
	print('Loading model from', model_path)
	qa_model = QuestionAnsweringModel('bert', model_path, use_cuda=use_cuda)
	return qa_model
```	



**âœ”ï¸ `sum_model.py`**<br />
- ìê¸°ì†Œê°œì„œë¥¼ í¬ê´„ì ìœ¼ë¡œ ìƒì„±ìš”ì•½í•˜ëŠ” ëª¨ë“ˆì´ë‹¤.<br />
- ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬:<br />
	- `transformers`: Hugging Faceì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬<br />

```Python	
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
def summarize_model(text: str, verbose: bool = True) -> str:
	tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')
	model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')
	input_ids = tokenizer.encode(text, return_tensors="pt")
	    summary_text_ids = model.generate(
	        input_ids=input_ids,
	        bos_token_id=model.config.bos_token_id,
	        eos_token_id=model.config.eos_token_id,
	        length_penalty=2.0,
	        max_length=102,
	        min_length=20,
	        num_beams=4,
	    )
	    summarized_text = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)
	    
	    return summarized_text
```
