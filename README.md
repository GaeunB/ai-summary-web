# ğŸ“‹  ìì†Œì„œ ìš”ì•½ë‹¨
**íŒ€ì›**: ë³€ê°€ì€, ê¹€ì§€í™˜, ë‚¨ê²½í˜„, ì´ëŒ€í—Œ, ì´ì„±ë¯¼, ìµœë¯¼ì¬<br />
[Notion](https://jungle-crane-580.notion.site/27be448bebcb4ca39ac9182033d7a293?pvs=4)
<br />

## ğŸ“ About
í”„ë¡œì íŠ¸ ê³„íš ì´ìœ ~

<br />ì „ì²´ êµ¬ì¡° ë° ì‘ë™ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 

## ğŸ”§ Architecture
**Frontend**: HTML,CSS,JS <br />
(í”„ë¡ íŠ¸ ìº¡ì³ í™”ë©´)<br />
**Engine**: KorQuAD, textrankr, kobart <br />
(ì „ë°˜ì ì¸ ë„ì‹í™”)<br />
**Backend**: Flask <br />

## Engine
#### âœ… kobart (ì›ë¦¬)
https://github.com/SKT-AI/KoBART <br /> 
BART(Bidirectional and Auto-Regressive Transformers)ëŠ” ì…ë ¥ í…ìŠ¤íŠ¸ ì¼ë¶€ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì´ë¥¼ ë‹¤ì‹œ ì›ë¬¸ìœ¼ë¡œ ë³µêµ¬í•˜ëŠ” autoencoderì˜ í˜•íƒœë¡œ í•™ìŠµì´ ë©ë‹ˆë‹¤.<br /> 
![image](https://github.com/Oneourbefore/ai-summary-web/assets/102707496/68021f7e-9981-4a97-9ae3-b3e6ff830132)
1. BartëŠ” Transformerì˜ ê¸°ë³¸ ì•„í‚¤í…ì²˜ì¸ Encoder-Decoderêµ¬ì¡°ë¥¼ ê°–ê³  ìˆë‹¤.
2. ë”°ë¼ì„œ ì½”ë“œë„ Encoderì™€ Decoderë¥¼ ì°¨ë¡€ë¡œ í†µê³¼í•œë‹¤.
3. Input dataë„ Encoder_inputê³¼ Decoder_inputì„ ë”°ë¡œ ì¤€ë¹„í•´ì•¼í•œë‹¤.
4. ì–´ë–»ê²Œ inputì„ ë„£ì–´ì£¼ëƒì— ë”°ë¼ Taskë§ˆë‹¤ í•™ìŠµ/ì¶”ë¡  ë°©ë²•ì´ ê°ˆë¦°ë‹¤.

í•œêµ­ì–´ BARTëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ Text Infilling ë…¸ì´ì¦ˆ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 40GB ì´ìƒì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ì— ëŒ€í•´ì„œ í•™ìŠµí•œ í•œêµ­ì–´ encoder-decoder ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. <br /> 
ì´ë¥¼ í†µí•´ ë„ì¶œëœ KoBART-baseë¥¼ ë°°í¬í•©ë‹ˆë‹¤. 
í•œêµ­ì–´ ìœ„í‚¤ ë°±ê³¼ ì´ì™¸, ë‰´ìŠ¤, ì±…, ëª¨ë‘ì˜ ë§ë­‰ì¹˜ v1.0(ëŒ€í™”, ë‰´ìŠ¤, ...), ì²­ì™€ëŒ€ êµ­ë¯¼ì²­ì› ë“±ì˜ ë‹¤ì–‘í•œ ë°ì´í„°ê°€ ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.

KoBARTë€ í˜ì´ìŠ¤ë¶ì—ì„œ ê³µê°œí•œ BARTëª¨ë¸ì„ SKTì—ì„œ 40GBì´ìƒì˜ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¡œ ì‚¬ì „ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì´ë‹¤.<br /> 
BARTëŠ” seq2seq ëª¨ë¸ì„ ì‚¬ì „í•™ìŠµí•˜ê¸° ìœ„í•œ denoising autoencoder(DAE, ì¡ìŒì œê±° ì˜¤í†  ì¸ì½”ë”)ë¡œ, ì„ì˜ì˜ noising functionìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì†ìƒì‹œí‚¨ í›„ ëª¨ë¸ì´ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ì¬êµ¬ì¶•í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ëœë‹¤.<br /> 
BARTëŠ” ê¸°ì¡´ BERTëª¨ë¸ê³¼ GPTë¥¼ í•©ì¹œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆëŠ”ë°, ì´ë¡œ ì¸í•´ BERTì˜ Bidirectional íŠ¹ì§•ê³¼ GPTì˜ Auto-Regressiveí•œ íŠ¹ì§•ì„ ëª¨ë‘ ê°€ì§„ë‹¤. ë•ë¶„ì— BARTëŠ” ê¸°ì¡´ MLMëª¨ë¸ë“¤ì— ë¹„í•´ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë†’ì€ í™œìš©ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤. 
![image](https://github.com/GaeunB/ai-summary-web/assets/118701576/f2e5c8e6-cedc-4ccf-b9f2-90f6ccc25501) <br /> 
Fig.1 Bartêµ¬ì¡°<br /> 
BARTëŠ” ì†ìƒëœ Textë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ Bidirectional ëª¨ë¸ë¡œ encodingí•˜ê³  ì •ë‹µ Textì— ëŒ€í•œ likelihoodë¥¼ autoregressive ëª¨ë¸ë¡œ decodingí•˜ì—¬ ê³„ì‚°í•œë‹¤. 
BARTì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ 5ê°€ì§€ noising ê¸°ë²•ì´ ì¡´ì¬í•œë©°, ì´ë¥¼ í†µí•´ ì†ìƒëœ Textë¥¼ ì–»ëŠ”ë‹¤.
![image](https://github.com/GaeunB/ai-summary-web/assets/118701576/d4db2cc4-2686-40e3-b4dd-38312d0c3ff1) <br /> 
Fig.2 Noisingê¸°ë²•<br /> 
BARTëŠ” ìê¸°íšŒê·€ ë””ì½”ë”ë¥¼ ê°–ê¸° ë•Œë¬¸ì—, abstractive QAì™€ summarizationê³¼ ê°™ì€ ì‹œí€€ìŠ¤ ì¼ë°˜í™”(Sequence Generation) íƒœìŠ¤í¬ì— ì§ì ‘ì ìœ¼ë¡œ íŒŒì¸íŠœë‹ ë  ìˆ˜ ìˆë‹¤. ì´ë²ˆ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì´ë ¥ì„œ ìš”ì•½ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ KoBARTëª¨ë¸ì— ì±„ìš©ë©´ì ‘ ë°ì´í„°ë¡œ íŒŒì¸íŠœë‹ì„ ì§„í–‰í•˜ì˜€ë‹¤.(ë°ì´í„°ì…‹: https://www.aihub.or.kr/aihubdata/data/view.do?currMenu=&topMenu=&aihubDataSe=realm&dataSetSn=71592) 
<br />

<details><summary>ì°¸ê³ ë¬¸í—Œ</summary>
[1] Mike Lewisì™¸(2019), "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", ACL <br /> 
[2] ìˆ˜ë‹¤ë¥´ì‚° ë¼ë¹„ì°¬ë””ë€(2021), "êµ¬ê¸€ BERTì˜ ì •ì„", í•œë¹›ë¯¸ë””ì–´
</details>


#### âœ… textrankr (ì›ë¦¬)
TextRank ì•Œê³ ë¦¬ì¦˜ì€ 2004ë…„ êµ¬ê¸€ì—ì„œ ë°œí‘œí•œ PageRank ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤[1].<br />
PageRank ì•Œê³ ë¦¬ì¦˜ì€ ìˆ˜ì§‘ëœ ì¸í„°ë„· ë¬¸ì„œ ê°ê°ì„ ê·¸ë˜í”„ì˜ ë…¸ë“œ, ë¬¸ì„œ ë‚´ë¶€ì˜ ë§í¬ ì •ë³´ë¥¼ ê°„ì„ ìœ¼ë¡œ ê°€ì •í•˜ì—¬ ë°©í–¥ì„±ì´ ìˆëŠ” ê·¸ë˜í”„ë¥¼ ë§Œë“¤ì–´ ë¬¸ì„œì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•œë‹¤[2]. ì¡°ê¸ˆ ë” ì‰½ê²Œ ë§í•˜ìë©´ PageRankëŠ” ê° ì›¹í˜ì´ì§€ë§ˆë‹¤ í•˜ì´í¼ë§í¬ê°€ ìˆì„ ë•Œ ì–¼ë§ˆë‚˜ ë§í¬ë¥¼ ë°›ëŠëƒì— ë”°ë¼ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ë§í•œë‹¤. ì¦‰, í•´ë‹¹ ë§í¬ë¥¼ í´ë¦­í•  í™•ë¥ ë¡œ ê·¸ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ê²ƒì´ë‹¤.<br />
TextRank ì•Œê³ ë¦¬ì¦˜ì€ PageRankì˜ ê°œë…ì„ ìì—°ì–´ ì²˜ë¦¬ì— ì‘ìš©í•œ ê²ƒìœ¼ë¡œ ë¬¸ì¥, ë‹¨ì–´ì™€ ê°™ì€ íŠ¹ì • ë‹¨ìœ„ë“¤ ê°„ì˜ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. ë¬¸ì„œ ë‚´ì˜ ê° ë¬¸ì¥ì„ ê·¸ë˜í”„ì˜ ì •ì (vertex)ìœ¼ë¡œ ê°€ì •í•˜ëŠ” ê²½ìš° ì¤‘ìš”í•œ ë¬¸ì¥ë“¤ì„ ì„ ë³„í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ ë¬¸ì„œ ìš”ì•½ì´ ê°€ëŠ¥í•˜ë‹¤. ê²°êµ­, TextRankëŠ” ì•ì„œ PageRankì—ì„œì˜ í˜ì´ì§€ ê°œë…ì„ ë‹¨ì–´ì˜ ê°œë…ìœ¼ë¡œ ë°”ê¾¼ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. í…ìŠ¤íŠ¸ë¡œ ì´ë£¨ì–´ì§„ ê¸€ì—ì„œ íŠ¹ì • ë‹¨ì–´ê°€ ë‹¤ë¥¸ ë¬¸ì¥ê³¼ ì–¼ë§ˆë§Œí¼ì˜ ê´€ê³„ë¥¼ ë§ºê³  ìˆëŠ”ì§€ë¥¼ ê³„ì‚°í•œë‹¤.<br />
<img width="450" alt="ìŠ¤í¬ë¦°ìƒ· 2024-02-20 ì˜¤ì „ 11 22 32" src="https://github.com/28sungmin/study/assets/115343559/e807a8ef-b229-404d-8be0-3059ce4304cd">
<br /> ìœ„ ì´ë¯¸ì§€ëŠ” ì£¼ì–´ì§„ ê¸€ì— ëŒ€í•´ í…ìŠ¤íŠ¸ ê°„ ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ë¡œ ê·¸ë¦° ìƒ˜í”Œ ì´ë¯¸ì§€ì´ë‹¤. ê° ë¬¸ì¥ì—ì„œ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ ì„ ìœ¼ë¡œ ì—°ê²°í•œ ê²ƒì´ë‹¤.<br />
ê·¸ë ‡ë‹¤ë©´ í•µì‹¬ ë¬¸ì¥ì„ ì¶”ì¶œí•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ? ì•„ë˜ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë¬¸ì¥ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ì–´ì•¼ í•œë‹¤. ê° ë¬¸ì¥ì´ ë§ˆë””ê°€ ë˜ëŠ” ê²ƒì´ë‹¤.
<img width="450" alt="ìŠ¤í¬ë¦°ìƒ· 2024-02-20 ì˜¤ì „ 11 28 52" src="https://github.com/28sungmin/study/assets/115343559/0e6bf9db-ab64-4ece-bac8-1b77cd2a277c">
<br />ì •ì„ì› ì™¸(2017) "TextRank ì•Œê³ ë¦¬ì¦˜ê³¼ ì£¼ì˜ ì§‘ì¤‘ ìˆœí™˜ ì‹ ê²½ë§ì„ ì´ìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¬¸ì„œ ìš”ì•½" ë…¼ë¬¸ì—ì„œëŠ” TextRank ì•Œê³ ë¦¬ì¦˜ì€ ê° ë¬¸ì¥ì˜ ì¤‘ìš”ë„ë¥¼ êµ¬í•  ë•Œ, ë¬¸ì¥ ê°„ ìƒê´€í–‰ë ¬ì„ ì´ìš©í•˜ì—¬ êµ¬í•˜ì˜€ë‹¤.
<img width="400" alt="textrank1" src="https://github.com/28sungmin/study/assets/115343559/b4f130bc-a2ac-40f8-bd2a-cf7a73ddd75c">
<br />ì…ë ¥ ë¬¸ì„œì˜ ê° ë¬¸ì¥ë“¤ì— ëŒ€í•´ í˜•íƒœì†Œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , ì²´ì–¸ë¥˜ì™€ ìš©ì–¸ë¥˜ì˜ TF-IDFë¥¼ ê³„ì‚°í•˜ì—¬ ë¬¸ì¥-ë‹¨ì–´ í–‰ë ¬ì„ ìƒì„±í•œë‹¤. ê·¸ ë’¤ ìƒì„±ëœ ë¬¸ì¥-ë‹¨ì–´ í–‰ë ¬ì˜ ì „ì¹˜ í–‰ë ¬ì„ êµ¬í•˜ì—¬ ì„œë¡œ ê³±í•´ì£¼ë©´ ë¬¸ì¥ ê°„ì˜ ìƒê´€ê´€ê³„(correlation)ì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ êµ¬í•œ ë¬¸ì¥ ê°„ ìƒê´€í–‰ë ¬ì€ ë¬¸ì¥ ê°„ì˜ ê°€ì¤‘ì¹˜ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê³ , TextRank ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ê° ë¬¸ì¥ì˜ ì¤‘ìš”ë„ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ êµ¬í•œ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ë¬¸ì¥ë“¤ì„ ì •ë ¬í•œ ë’¤ ìƒìœ„ nê°œì˜ ë¬¸ì¥ë“¤ì„ ì¬ë°°ì¹˜í•˜ë©´ ìš”ì•½ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤[3].<br />

<br />
<details><summary>ì°¸ê³ ë¬¸í—Œ</summary>
[1] ì´ìƒì˜ ì™¸(2023), "TextRank ì•Œê³ ë¦¬ì¦˜ ë° ì¸ê³µì§€ëŠ¥ì„ í™œìš©í•œ ë¸Œë ˆì¸ìŠ¤í† ë°", JPEE : Journal of practical engineering education = ì‹¤ì²œê³µí•™êµìœ¡ë…¼ë¬¸ì§€, v.15 no.2, pp.509 - 517 <br />
[2] ë°°ì›ì‹ê³¼ ì°¨ì •ì›(2010), "TextRank ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ë¬¸ì„œ ë²”ì£¼í™”", ì •ë³´ê³¼í•™íšŒë…¼ë¬¸ì§€. Journal of KIISE. ì»´í“¨íŒ…ì˜ ì‹¤ì œ ë° ë ˆí„°, v.16 no.1, pp.110-114 <br />
[3] ì •ì„ì› ì™¸(2017), "TextRank ì•Œê³ ë¦¬ì¦˜ê³¼ ì£¼ì˜ì§‘ì¤‘ ìˆœí™˜ ì‹ ê²½ë§ì„ ì´ìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë¬¸ì„œ ìš”ì•½", í•œêµ­ì–´ì •ë³´í•™íšŒ 2017ë…„ë„ ì œ29íšŒ í•œê¸€ë°í•œêµ­ì–´ì •ë³´ì²˜ë¦¬í•™ìˆ ëŒ€íšŒ, pp.47 - 50 <br />
</details>


#### âœ… KorQuAD (ì›ë¦¬)



## ì£¼ìš”ê¸°ëŠ¥
- **General Summarize**: ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì„ í™œìš©í•œ ìê¸°ì†Œê°œì„œ ìš”ì•½
- **Keysentence Extraction**: í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ë¬¸ì¥ ì¶”ì¶œ
- **Question Answering**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì œê³µ 
 
## ğŸ“‘ Code Architecture
**`app.py`**<br />
- `app.py` ì—ì„œëŠ” ì—”ì§„ì„ ì´ˆê¸° ì„¸íŒ…í•˜ê³ , flask ì„œë²„ ì„¤ì •ì„ í†µí•´ í”„ë¡ íŠ¸ì™€ í†µì‹ ì„ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤. ì—”ì§„ì—ì„œ ê°€ì¥ ì¤‘ì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ” ì½”ë“œ íŒŒì¼ì´ë©°, ë°±ì—”ë“œì—ì„œ ìš”ì²­ì´ ë“¤ì–´ì˜¬ ê²½ìš°, ê·¸ì— ë§ëŠ” ì‘ì—…ì„ ìˆ˜í–‰í•˜ì—¬ ì²˜ë¦¬í•œë‹¤.<br />
- flask ì•±ì„ ìƒì„±í•˜ê³  ë‹¤ìŒ ê²½ë¡œì— ëŒ€í•œ ë¼ìš°íŒ…ì„ ì„¤ì •í•œë‹¤.
	- `'/'`: í™ˆ í˜ì´ì§€ë¥¼ ë Œë”ë§
	- `'/sum'`: ìš”ì•½ í˜ì´ì§€ë¥¼ ë Œë”ë§
	- `'/sum/gsummarize'`: ì¼ë°˜ì ì¸ ìš”ì•½ì„ ìˆ˜í–‰
	- `'/sum/key'`: í‚¤ì›Œë“œ ì¤‘ì‹¬ì˜ ë¬¸ì¥ ì¶”ì¶œì„ ìˆ˜í–‰
	- `'/sum/qa'`: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µ
- Flask-CORSë¥¼ í™œìš©í•˜ì—¬ CORS ì´ìŠˆë¥¼ í•´ê²°í•œë‹¤.

<details><summary>code</summary>

```Python
from flask import Flask,render_template,request, jsonify
from flask_cors import CORS
import torch
from sum_model import summarize_model
from ext import textrank_summarize
from qa_model import get_qa_model

app = Flask(__name__)

cors = CORS(app)

#home
@app.route('/')	
def home():
	return render_template('home.html')

#summary page
@app.route('/sum')	
def index():
	return render_template('index.html')

#general summarization
@app.route('/sum/gsummarize', methods=['POST'])
def gsummarize():
	try:
		data = request.get_json(force=True)
		context = data['context']
		gsum = summarize_model(context)
		response = jsonify({'gsum': gsum})

	except Exception as e:
		response = jsonify({'error': str(e)})	
	return response


# keysentence extraction
@app.route('/sum/key',methods=['POST'])
def key():
	try:
		data = request.get_json(force=True)
		context = data['context']
		keytext = textrank_summarize(context,1) #ë¬¸ì¥ ìˆ˜ ì¡°ì ˆ í•„ìš”
		response = jsonify({'keytext': keytext})

	except Exception as e:
		response = jsonify({'error': str(e)})	
		
	return response
	
#qa
@app.route('/sum/qa', methods=['POST'])
def qa_endpoint():
	try:
		data = request.get_json(force=True)

		context = data['context']
		question = data['question']
		if question == "":
			response = jsonify({'error': 'ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.'})
			return response

		to_predict = [{"context": context, "qas": [{"question": question, "id": "0"}]}]

		result = qa_model.predict(to_predict)

		answer = result[0][0]['answer'][0]
		answer = "ì ì ˆí•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤." if answer == '' else answer

		response = jsonify({'answer': answer})

	except Exception as e:
		response = jsonify({'error': str(e)})

	return response

if __name__ == '__main__':
	model_path = 'model/checkpoint-1119-epoch-1'
	qa_model = get_qa_model(model_path, use_cuda=False)
	
    # general summarization
    # summodel_path = 'model/model.pt'
    # sum_model=get_sum_model(summodel_path)
    # sum_model.load_state_dict(torch.load(summodel_path), strict=False)
    # sum_model.eval()

	app.run(host='127.0.0.1',port=5000,debug=True)

```
</details>


**`ext.py`**<br />
- í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì¤‘ìš”ë¬¸ì¥ì„ ì¶”ì¶œí•˜ëŠ”ëŠ” ëª¨ë“ˆì´ë‹¤.<br />
- ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬:<br />
`konlpy`: í•œêµ­ì–´ ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
`textrankr`: TextRank ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•œ í…ìŠ¤íŠ¸ ìš”ì•½ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬

<details><summary>code</summary>

```Python
from typing import List
from konlpy.tag import Okt
from textrankr import TextRank

class OktTokenizer:
    okt: Okt = Okt()

    def __call__(self, text: str) -> List[str]:
        tokens: List[str] = self.okt.phrases(text)
        return tokens

def textrank_summarize(text: str, num_sentences: int, verbose: bool = True) -> str:
    mytokenizer: OktTokenizer = OktTokenizer()
    textrank: TextRank = TextRank(mytokenizer)
    summarized: str = textrank.summarize(text, num_sentences, verbose)
    return summarized

# if verbose = False, it returns a list
# summaries: List[str] = textrank.summarize(text, 3, verbose=False)
# for summary in summaries:
#     print(summary)
```
</details>


**`qa_model.py`**<br />
- ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ëª¨ë“ˆì´ë‹¤.<br />
- ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬:
	- `simpletransformers`: ê°„í¸í•œ ì‚¬ìš©ì„ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬

<details><summary>code</summary>
 
```Python
import simpletransformers

from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs

def get_qa_model(model_path, use_cuda=False):
    print('Loading model from', model_path)
    qa_model = QuestionAnsweringModel('bert', model_path, use_cuda=use_cuda)
    return qa_model
```
</details>


**`sum_model.py`**<br />
- ìê¸°ì†Œê°œì„œë¥¼ í¬ê´„ì ìœ¼ë¡œ ìƒì„±ìš”ì•½í•˜ëŠ” ëª¨ë“ˆì´ë‹¤.<br />
- ì‚¬ìš©ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬:<br />
	- `transformers`: Hugging Faceì˜ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬

<details><summary>code</summary>
	
```Python
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
def summarize_model(text: str, verbose: bool = True) -> str:
    tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')
    model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')
    input_ids = tokenizer.encode(text, return_tensors="pt")
    summary_text_ids = model.generate(
        input_ids=input_ids,
        bos_token_id=model.config.bos_token_id,
        eos_token_id=model.config.eos_token_id,
        length_penalty=2.0,
        max_length=102,
        min_length=20,
        num_beams=4,
    )
    summarized_text = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)
    
    return summarized_text
```
</details>
